---
title: "Class 7: Machine Learning 1"
author: "Raquel Gonzalez (PID: A16207442)"
format: pdf
editor: visual
---

# Clustering Methods

The broad goal here is to find groupings (clusters) in your input data.

## Kmeans

First, let's make up some data to cluster.

```{r}
x <- rnorm(1000)
hist(x)
```

Make a vector of length 60 with 30 points centered at -3 and 30 points centered at +3.

```{r}
tmp <- c(rnorm(30, mean=-3), rnorm(30, mean=3))
```

I will now make a wee x and y dataset with 2 groups of points

```{r}
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

```{r}
k <- kmeans(x, centers=2)
k
```

> Q. From your result object 'k', how many points are in each cluster?

```{r}
k$size
```

> Q. What "component" of your result object details the cluster membership?

```{r}
k$cluster
```

> Q. Cluster centers?

```{r}
k$centers
```

> Q. Plot of our clustering results

```{r}
plot(x, col=k$cluster)
points(k$centers, col="blue", pch=15, cex=2)
```

We can cluster data into 4 groups.

```{r}
# kmeans
k4 <- kmeans(x, centers=4)
# plot results
plot(x, col=k4$cluster)
```

A big limitation of kmeans is that it does what you ask even if you ask for silly clusters.

## Hierarchical Clustering

The main base R function for Hierarchical Clustering is `hclust()`. Unlike `kmeans()`, you cannot just pass it your data as input. You first need to calculate a distance matrix.

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

Use `plot()` to view results

```{r}
plot(hc)
abline(h=10, col="red")
```

To make the "cut" and get our cluster membership vector we can use the `cutree()` function

```{r}
grps <- cutree(hc, h=10)
grps
```

Make a plot of our data colored by hclust results.

```{r}
plot(x, col=grps)
```

# Principal Component Analysis (PCA)

Here we will do Principal Component Analysis (PCA) on some food data from the UK.

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1)
```

```{r}
#rownames(x) <-  x[,1]
#x <- x[, -1]
#x
```

> Q1. How many rows and columns are in your new data frame named `x`? What R functions could you use to answer this question?

```{r}
dim(x)
```

```{r}
##Preview the first 6 rows
head(x)
```

> Q2: Which approach to solving the 'row-names problem' mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

Setting the row.names argument is preferred, because the `rownames()` function will consistently delete the first row each time you run it.

> Q3: Changing what optional argument in the above `barplot()` function results in the following plot?

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

Changing `beside=T` to `beside=F`results in the correct barplot.

> Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

If a point is on the diagonal, that means the values are all very similar (i.e. match the average). If a point is far from that diagonal, this means it deviates from the norm. 

```{r}
pairs(x, col=rainbow(10), pch=16)
```

If a point lies on the diagonal, 

> Q6: What is the main difference between N.Ireland and the other countries of the UK in terms of this data-set?

N. Ireland tends to consume less alcoholic beverages and fresh fruit and consumes more fresh potatoes and soft drinks.

## PCA to the rescue

The main "base" R function for PCA is called `prcomp()`. Here we need to take the transpose of the data to switch the rows and columns. 

```{r}
pca <- prcomp( t(x) )
summary(pca)
```

> Q: How much variance is captured in 2 PCs

96.5%

To make our main "PC score plot" (aka or "PC1 vs PC2 plot" or "PC plot" or "Coordination plot").

```{r}
attributes(pca) 
```

We are after the `pca$x` result component to make our main PCA plot. 

```{r}
mycols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, xlab="PC1 (67.4%)", ylab="PC2 (29%)")
```

Another important result from PCA is how the original variables (in this case: the foods) contribute to the PCs.

This is contained in the `pca$rotation` object - folks often call this the "loadings" or "contributions" to the PCs. 

```{r}
pca$rotation
```

We can make a plot along PC1.

```{r}
library(ggplot2)

contrib <- as.data.frame(pca$rotation)

ggplot(contrib) + 
  aes(PC1, rownames(contrib)) +
  geom_col()
```

